# Molecular Subtype Classifier (METABRIC PAM50)

A modular machine learning pipeline that classifies breast cancer tumors into **PAM50 molecular subtypes** using **numeric genomic + clinical features** from the **METABRIC** dataset.

The project includes:
- leakage-safe preprocessing (fit on train only),
- training + evaluation for two baseline models (Logistic Regression, Random Forest),
- persisted artifacts (models + preprocessing metadata),
- evaluation reports (metrics JSON + labeled confusion matrices),
- feature importance exports,
- interactive query tabs for filtering data, metrics, and feature scores,
- a password-protected **Streamlit dashboard** for visualization + CSV-based prediction,
- and structured application logging for monitoring and maintenance.

---

## Models

- **Logistic Regression** (multiclass)
- **Random Forest** (multiclass)

---

## Dataset

- **Input file:** `data/raw/METABRIC_RNA_Mutation.csv`
- **Target column:** `pam50_+_claudin-low_subtype` (configured in `src/config.py`)
- **Cleaned output files (generated by CLI `all`):**
  - `data/cleaned/metabric_clean_train.csv`
  - `data/cleaned/metabric_clean_test.csv`

Cleaning behavior:
- Drops rows with missing target labels
- Drops the rare `NC` class by default (insufficient sample size)

---

## Project layout

```
molecular-subtype-classifier/
├── .streamlit/
│   └── secrets.toml           (local development only, not committed)
├── data/
│   ├── raw/
│       └── METABRIC_RNA_Mutation.csv
│   └── cleaned/
│       ├── metabric_clean_train.csv
│       └── metabric_clean_test.csv
├── artifacts/
│   ├── models/
│   │   ├── logistic_regression.joblib
│   │   ├── random_forest.joblib
│   │   └── preprocess_artifacts.joblib
│   ├── reports/
│   │   ├── metrics.json
│   │   ├── confusion_matrix_logistic_regression.csv
│   │   ├── confusion_matrix_random_forest.csv
│   │   ├── feature_importance_random_forest.csv
│   │   └── feature_importance_logistic_regression.csv
│   └── logs/
│       └── app.log
├── src/
│   ├── config.py
│   ├── main.py
│   ├── data/
│   │   ├── load_data.py
│   │   ├── preprocess.py
│   │   └── split.py
│   ├── modeling/
│   │   ├── train.py
│   │   ├── evaluate.py
│   │   ├── explain.py
│   │   └── predict.py
│   └── viz/
│       └── plots.py
├── streamlit_app.py
├── requirements.txt
└── README.md
```

---

## Installation

From the project root:

```bash
pip install -r requirements.txt
```

---

## Security (Password Authentication)

The Streamlit dashboard includes a login gate to restrict access.

Authentication features:

- Password stored securely using **Streamlit Secrets**
- Password not hard-coded in source code
- Authentication state managed via `st.session_state`
- Unauthorized users are blocked from accessing data and predictions

### Local development setup

Create a file:

```
.streamlit/secrets.toml
```

With:

```toml
APP_PASSWORD = "your-password-here"
```

Ensure this file is excluded via `.gitignore`.

When deploying to Streamlit Cloud, the same `APP_PASSWORD` variable is configured in the **Secrets** panel instead of committing the file.

---

## Monitoring & Maintenance

The application includes lightweight monitoring features:

- Structured logging using Python’s `logging` module
- Logs written to both console and `artifacts/logs/app.log`
- Logs capture:
  - CLI training and evaluation runs
  - Successful and failed login attempts
  - Model selection events
  - Prediction CSV upload events
  - Prediction execution
  - Exception traces

The dashboard sidebar also displays artifact availability checks (e.g., presence of `metrics.json`) to support operational validation.

These features support maintainability and operational transparency.

---

## Preprocessing

Preprocessing is fit on training data only and then reused for test and prediction.

### Feature selection
- Uses **numeric columns only**
- Excludes the target column if numeric

### Missing value handling
- Computes **median values on TRAIN only**
- Imputes missing values using those training medians for:
  - train transformation
  - test transformation
  - prediction transformation

### Scaling
- Fits a `StandardScaler` on **TRAIN only**
- Applies the trained scaler to test/prediction

### Label encoding
- Fits `LabelEncoder` on **TRAIN labels**
- Transforms test labels with the train-fitted encoder

All preprocessing metadata is persisted in:

```
artifacts/models/preprocess_artifacts.joblib
```

---

## Train / Evaluate (CLI)

The pipeline is driven by `src/main.py`.

### Train models

```bash
python3 -m src.main train
```

### Evaluate saved models

```bash
python3 -m src.main eval
```

Evaluate only one model:

```bash
python3 -m src.main eval --model random_forest
```

### Train + Eval (one-shot)

```bash
python3 -m src.main all
```

This command also exports cleaned train/test splits for reporting and traceability:

```bash
data/cleaned/metabric_clean_train.csv
data/cleaned/metabric_clean_test.csv
```

Useful flags:
- `--seed` (default: 42)
- `--test-size` (default: 0.2)
- `--no-stratify` (disable stratified split)
- `--explain` (also save feature importance CSV)
- `--top-n` (default: 20)

Example:

```bash
python3 -m src.main all --explain --top-n 30
```

---

## Outputs / Artifacts

### Models

```
artifacts/models/
  logistic_regression.joblib
  random_forest.joblib
  preprocess_artifacts.joblib
```

### Evaluation reports

```
artifacts/reports/
  metrics.json
  confusion_matrix_<model>.csv
```

`metrics.json` includes:
- accuracy
- macro F1
- weighted F1
- classification report (dictionary format)
- confusion matrix (raw + labeled forms)

### Explainability

When `--explain` is used, the CLI writes feature-importance exports for both models:

```
artifacts/reports/feature_importance_random_forest.csv
artifacts/reports/feature_importance_logistic_regression.csv
```

---

## Streamlit Dashboard

The Streamlit app provides:

- dataset summary + subtype distribution
- model metrics comparison and confusion matrix visualization
- descriptive PCA visualization (**not used for training**)
- feature importance plots
- CSV upload prediction with optional probability outputs
- password-protected access
- sidebar monitoring indicators

Run locally:

```bash
streamlit run streamlit_app.py
```

If artifacts are missing, generate them first:

```bash
python3 -m src.main all --explain
```

---

## Programmatic Prediction

```python
import pandas as pd
from src.modeling.predict import predict_from_dataframe

df = pd.read_csv("my_samples.csv")
preds = predict_from_dataframe(df, model_name="random_forest", return_proba=True)
print(preds.head())
```

Output includes:
- `predicted_label` (string subtype)
- `predicted_label_id` (optional integer encoding)
- `proba_<class>` columns when supported by the model

---

## Reproducibility

- Train/test splitting uses `train_test_split`
- Default test size: 20%
- Stratification enabled by default (disable with `--no-stratify`)
- Random seed configurable via `--seed`
- Models use fixed `random_state` where supported

---

## Author

Lauren Biles  
B.S. Computer Science — Western Governors University  
